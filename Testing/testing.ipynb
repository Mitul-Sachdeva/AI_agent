{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import Settings\n",
    "import re\n",
    "load_dotenv()\n",
    "llm = Gemini(api_key=os.environ[\"GOOGLE_API_KEY\"],model=\"models/gemini-pro\")\n",
    "embed_model = GeminiEmbedding(api_key=os.environ[\"GOOGLE_API_KEY\"],model_name=\"models/embedding-001\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "\n",
    "# Load data from PDF\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"testdata\").load_data()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "em=embed_model.get_text_embedding(\"hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_Settings(_llm=Gemini(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D4CF00B220>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001D4E76DCAF0>, completion_to_prompt=<function default_completion_to_prompt at 0x000001D4E77ED3F0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='models/gemini-pro', temperature=0.1, max_tokens=2048, generate_kwargs={}), _embed_model=GeminiEmbedding(model_name='models/embedding-001', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D4CF00B220>, num_workers=None, title=None, task_type='retrieval_document', api_key='AIzaSyDrefuYMMd23jgptIbmyu7ZbdggrREDfWI'), _callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D4CF00B220>, _tokenizer=None, _node_parser=SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D4CF00B220>, id_func=<function default_id_func at 0x000001D4E785A440>, chunk_size=512, chunk_overlap=20, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?'), _prompt_helper=None, _transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D4CF00B220>, id_func=<function default_id_func at 0x000001D4E785A440>, chunk_size=512, chunk_overlap=20, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "pinecone_index = pinecone_client.Index(\"constitution\")\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 2/2 [00:00<00:00, 12.78it/s]\n",
      "Generating embeddings: 100%|██████████| 226/226 [01:45<00:00,  2.14it/s]\n",
      "Upserted vectors: 100%|██████████| 226/226 [00:05<00:00, 44.88it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an index from the documents and save it to the disk.\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context,show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = Gemini(api_key=os.environ[\"GOOGLE_API_KEY\"],model=\"models/gemini-1.5-pro-002\")\n",
    "embed_model = GeminiEmbedding(api_key=os.environ[\"GOOGLE_API_KEY\"],model_name=\"models/embedding-001\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "pinecone_client = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "pinecone_index = pinecone_client.Index(\"constitution\")\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "\n",
    "# Get the index from the vector store\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collections': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    "    )\n",
    "\n",
    "DEFAULT_TEXT_QA_PROMPT = PromptTemplate(\n",
    "        DEFAULT_TEXT_QA_PROMPT_TMPL, \n",
    "        prompt_type=PromptType.QUESTION_ANSWER\n",
    "        \n",
    "    )\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 14 prohibits discrimination on the grounds of religion, race, caste, sex, or place of birth.  It ensures equal access to public places and amenities maintained by the state.  It also allows for special provisions for women, children, and socially and educationally backward classes, including the Scheduled Castes and Scheduled Tribes.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is article 14\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The right to equality is a fundamental human right that recognizes the inherent dignity and equal worth of all individuals.  It means that everyone is entitled to the same rights and freedoms, regardless of their race, color, sex, language, religion, political or other opinion, national or social origin, property, birth, or other status.  This includes protection from discrimination in all areas of life, including:\n",
      "\n",
      "* **Before the law:** Everyone is equal before the law and is entitled to equal protection of the law without any discrimination. This means equal access to justice, fair trials, and equal treatment by law enforcement and the judicial system.\n",
      "* **Equal opportunity:**  Everyone should have equal opportunities in education, employment, healthcare, housing, and other areas of life. This doesn't necessarily mean identical outcomes, but it does mean that no one should be denied opportunities based on discriminatory factors.\n",
      "* **Social equality:** This aspect aims to eliminate social hierarchies and discrimination based on factors like caste, ethnicity, or gender. It promotes inclusivity and respect for diversity.\n",
      "* **Equal treatment and protection from discrimination:** This is the core of the right to equality. It ensures that individuals are treated with dignity and respect and are not subjected to discriminatory practices or policies.\n",
      "\n",
      "It's important to note that the right to equality doesn't mean everyone is the same or should be treated identically in all situations.  It recognizes that people have different needs and circumstances and that sometimes different treatment is necessary to achieve genuine equality.  For example, affirmative action policies are designed to address historical disadvantages faced by certain groups and promote equality of opportunity.\n",
      "\n",
      "The right to equality is enshrined in numerous international human rights instruments, including the Universal Declaration of Human Rights and the International Covenant on Civil and Political Rights. Many countries also have constitutional provisions and laws that protect the right to equality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = llm.complete(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to testdata\\Constitution(updated).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"testdata\\Constitution Of India.csv\"  # Replace with the path to your input CSV file\n",
    "output_file = \"testdata\\Constitution(updated).csv\"  # Replace with the desired output file path\n",
    "\n",
    "# Common word to add\n",
    "common_word = \"Article\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file, header=None, names=[\"Text\"])\n",
    "\n",
    "# Add the common word to each row\n",
    "df[\"Text\"] = common_word + \" \" + df[\"Text\"]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV\n",
    "df.to_csv(output_file, index=False, header=False)\n",
    "\n",
    "print(f\"Updated CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file, header=None, names=[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text\n",
      "0    Article 1. Name and territory of the Union\\n(1...\n",
      "1    Article 1. The territories of the States; the ...\n",
      "2    Article 2. Admission or establishment of new S...\n",
      "3    Article 2A. Sikkim to be associated with the U...\n",
      "4    Article 3. Formation of new States and alterat...\n",
      "..                                                 ...\n",
      "451  Article 378A. Special provision as to duration...\n",
      "452  Article 392. Power of the President to remove ...\n",
      "453  Article 393. Short title This Constitution may...\n",
      "454  Article 394. Commencement This article and Art...\n",
      "455  Article 395. Repeals The Indian Independence A...\n",
      "\n",
      "[456 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
